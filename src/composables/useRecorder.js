import { onMounted, onUnmounted, ref } from 'vue';
import { OpenAITranscription } from '../utils/openaiTranscription.js';

// Composable OTIMIZADO: grava √°udio do sistema + microfone e usa OpenAI Whisper + Gemini para transcrever e resumir
export function useRecorder() {
  const isRecording = ref(false);
  const isProcessing = ref(false);
  const transcript = ref('');
  const error = ref(null);
  const isSupported = ref(false);
  const hasAudio = ref(false);
  const audioBlob = ref(null);

  // Estado da captura de √°udio
  const audioCaptureType = ref('unknown'); // 'system', 'microfone', 'hybrid', 'unknown'
  const isCapturingFullMeeting = ref(false); // true se capturando √°udio de todos
  const audioSources = ref([]); // fontes dispon√≠veis

  // Detalhes espec√≠ficos da captura
  const isCapturingInput = ref(false); // microfone/entrada
  const isCapturingOutput = ref(false); // sistema/sa√≠da
  const audioQuality = ref({ input: 0, output: 0 }); // n√≠veis de √°udio detectados
  const detectedMeetingApp = ref(''); // Teams, Meet, Zoom detectado


  let mediaRecorder = null;
  let audioChunks = [];
  let startTime = null;
  let removeElectronListener = null;
  // Configura√ß√µes de API - Whisper como prioridade
  const GEMINI_API_KEY = import.meta.env.VITE_GOOGLE_API_KEY;
  let OPENAI_API_KEY = import.meta.env.VITE_OPENAI_API_KEY || localStorage.getItem('openai_api_key') || '';
  let OPENAI_ORG_ID = localStorage.getItem('openai_org_id') || null;

  let openaiTranscription = null;
  let audioAnalyzer = null;
  let analysisInterval = null;

  const checkSupport = () => {
    isSupported.value = !!(navigator.mediaDevices && window.MediaRecorder);
    if (!isSupported.value) error.value = 'Captura de √°udio n√£o suportada neste ambiente.';
    return isSupported.value;
  };

  // Fun√ß√£o SEGURA para analisar qualidade do √°udio em tempo real
  const analyzeAudioStream = (stream) => {
    // DESABILITADO TEMPORARIAMENTE para evitar crashes
    console.log('üîä An√°lise de √°udio DESABILITADA (evitando crashes)');
    return;

  };

  // Fun√ß√£o para parar an√°lise
  const stopAudioAnalysis = () => {
    if (analysisInterval) {
      clearInterval(analysisInterval);
      analysisInterval = null;
    }
    if (audioAnalyzer) {
      audioAnalyzer.audioContext.close();
      audioAnalyzer = null;
    }
  };

  // Fun√ß√£o para detectar aplicativo de reuni√£o ativo
  const detectMeetingApp = async () => {
    const userAgent = navigator.userAgent.toLowerCase();
    const currentUrl = window.location.href.toLowerCase();

    // Detecta por URL (quando executado em browser)
    if (currentUrl.includes('teams.microsoft.com') || currentUrl.includes('teams.live.com')) {
      detectedMeetingApp.value = 'Microsoft Teams';
      return 'teams';
    } else if (currentUrl.includes('meet.google.com')) {
      detectedMeetingApp.value = 'Google Meet';
      return 'meet';
    } else if (currentUrl.includes('zoom.us')) {
      detectedMeetingApp.value = 'Zoom';
      return 'zoom';
    }

    // Detecta por t√≠tulo da janela (quando executado em Electron)
    if (window.electronAPI && window.electronAPI.getMeetingDebug) {
      try {
        const debug = await window.electronAPI.getMeetingDebug();
        const title = debug.lastMeetingTitle?.toLowerCase() || '';

        if (title.includes('teams') || title.includes('microsoft teams')) {
          detectedMeetingApp.value = 'Microsoft Teams';
          return 'teams';
        } else if (title.includes('meet') || title.includes('google meet')) {
          detectedMeetingApp.value = 'Google Meet';
          return 'meet';
        } else if (title.includes('zoom')) {
          detectedMeetingApp.value = 'Zoom';
          return 'zoom';
        }
      } catch (error) {
        console.warn('Falha ao detectar app de reuni√£o:', error);
      }
    }

    return 'unknown';
  };

  // Vari√°veis para streams separados (como Notion)
  let microphoneStream = null;
  let systemAudioStream = null;
  let audioContext = null;

  const startRecording = async () => {
    if (!checkSupport() || isRecording.value) return;
    console.log('üé¨ Iniciando captura: system(loopback) + microfone');
    try {
      // Captura do sistema (mant√©m l√≥gica atual funcionando) ‚Äì prioriza handler loopback do preload
      let systemStream = null;
      if (window.electronAPI?.captureSystemAudio) {
        try {
          const s = await window.electronAPI.captureSystemAudio();
          if (s && typeof s.getAudioTracks === 'function' && s.getAudioTracks().length) {
            systemStream = s;
            console.log('üñ•Ô∏è Loopback OK:', s.getAudioTracks()[0].label);
          }
        } catch (e) {
          console.warn('‚ö†Ô∏è loopback falhou:', e.message);
        }
      }
      // Tentativa 1: somente √°udio (ideal para handler)
      if (!systemStream) {
        try {
          console.log('üß™ Tentativa 1: getDisplayMedia({audio:true, video:false})');
          const pureAudio = await navigator.mediaDevices.getDisplayMedia({ audio: true, video: false });
          if (pureAudio.getAudioTracks().length) {
            systemStream = pureAudio;
            console.log('ÔøΩÔ∏è System audio (pure) OK:', systemStream.getAudioTracks()[0].label);
          } else {
            pureAudio.getTracks().forEach(t => t.stop());
          }
        } catch (e) {
          console.warn('‚ö†Ô∏è Tentativa 1 falhou:', e.message);
        }
      }
      if (!systemStream) {
        console.log('ÔøΩüîÑ Fallback loopback via getDisplayMedia...');
        try {
          const fallback = await navigator.mediaDevices.getDisplayMedia({ video: true, audio: true });
          fallback.getVideoTracks().forEach(t => { t.stop(); fallback.removeTrack(t); });
          if (fallback.getAudioTracks().length) {
            systemStream = fallback;
            console.log('üñ•Ô∏è Fallback system OK:', systemStream.getAudioTracks()[0].label);
          }
        } catch (e) {
          console.warn('‚ö†Ô∏è getDisplayMedia sem system audio:', e.message);
        }
      }
      if (!systemStream) {
        console.warn('‚ùå Nenhum √°udio do sistema obtido ap√≥s todas as tentativas. Prosseguindo s√≥ com microfone.');
      }

      // Captura microfone
      try {
        microphoneStream = await navigator.mediaDevices.getUserMedia({
          audio: {
            echoCancellation: true,
            noiseSuppression: true,
            autoGainControl: true
          }
        });
        console.log('üé§ Microfone OK:', microphoneStream.getAudioTracks()[0]?.label);
        isCapturingInput.value = true;
      } catch (e) {
        console.warn('‚ö†Ô∏è Microfone indispon√≠vel:', e.message);
        microphoneStream = null;
      }

      if (systemStream) {
        systemAudioStream = systemStream;
        isCapturingOutput.value = true;
        // Heur√≠stica: alguns ambientes retornam track do pr√≥prio mic como "default" / "communications" quando loopback falha
        const label = systemStream.getAudioTracks()[0]?.label?.toLowerCase() || '';
        if (label && (label.includes('microphone') || label.includes('mic') || label.includes('input'))) {
          console.warn('‚ö†Ô∏è A track de "system" parece ser microfone (label heur√≠stica).');
        }
      }

      if (!systemAudioStream && !microphoneStream) {
        error.value = 'Nenhuma fonte de √°udio dispon√≠vel';
        return;
      }

      let finalStream;
      if (systemAudioStream && microphoneStream) {
        audioCaptureType.value = 'hybrid';
        isCapturingFullMeeting.value = true;
        finalStream = await mixStreamsHybrid(microphoneStream, systemAudioStream);
        console.log('‚úÖ H√≠brido ativo (mix system + mic)');
      } else if (systemAudioStream) {
        audioCaptureType.value = 'system';
        isCapturingFullMeeting.value = true;
        finalStream = systemAudioStream;
        console.log('üéß Apenas system audio');
      } else {
        audioCaptureType.value = 'microphone';
        isCapturingFullMeeting.value = false;
        finalStream = microphoneStream;
        console.log('üé§ Apenas microfone');
      }

      await setupRecording(finalStream);
    } catch (e) {
      console.error('‚ùå Erro na captura combinada:', e);
      await cleanupStreams();
      throw e;
    }
  };

  // Fun√ß√£o para limpar recursos
  const cleanupStreams = async () => {
    console.log('üßπ Limpando streams...');

    stopAudioAnalysis();

    if (microphoneStream) {
      microphoneStream.getTracks().forEach(track => track.stop());
      microphoneStream = null;
    }

    if (systemAudioStream) {
      systemAudioStream.getTracks().forEach(track => track.stop());
      systemAudioStream = null;
    }

    if (audioContext && audioContext.state !== 'closed') {
      await audioContext.close();
      audioContext = null;
    }

    resetCaptureState();
  };

  // Mixagem h√≠brida (microfone + sistema) com Web Audio
  const mixStreamsHybrid = async (micStream, sysStream) => {
    try {
      if (!micStream || !sysStream) return micStream || sysStream;

      const AudioContextClass = window.AudioContext || window.webkitAudioContext;
      if (!AudioContextClass) {
        console.warn('‚ö†Ô∏è AudioContext indispon√≠vel, usando fallback simples.');
        // Fallback: junta tracks em novo MediaStream (se suportado)
        const merged = new MediaStream();
        micStream.getAudioTracks().forEach(t => merged.addTrack(t));
        sysStream.getAudioTracks().forEach(t => merged.addTrack(t));
        return merged;
      }

      audioContext = new AudioContextClass({ latencyHint: 'interactive' });
      const destination = audioContext.createMediaStreamDestination();

      const micSource = audioContext.createMediaStreamSource(micStream);
      const sysSource = audioContext.createMediaStreamSource(sysStream);

      // Ganhos equilibrados (sistema normalmente j√° vem mais alto)
      const micGain = audioContext.createGain();
      const sysGain = audioContext.createGain();
      micGain.gain.value = 1.2; // Leve boost no microfone
      sysGain.gain.value = 0.9; // Leve redu√ß√£o no sistema

      micSource.connect(micGain).connect(destination);
      sysSource.connect(sysGain).connect(destination);

      console.log('üîó Mixagem h√≠brida criada (mic + system)');
      return destination.stream;
    } catch (e) {
      console.error('‚ùå Falha na mixagem h√≠brida:', e.message);
      // Fallback: retorna system se existir, sen√£o mic
      return sysStream || micStream;
    }
  };

  // Fun√ß√£o simplificada para configurar MediaRecorder (m√©todo Notion)
  const setupRecording = async (stream) => {
    try {
      if (!stream || stream.getAudioTracks().length === 0) {
        throw new Error('Stream de √°udio inv√°lido');
      }

      console.log('üé¨ Configurando grava√ß√£o (tipo:', audioCaptureType.value, ')...');

      // MIME type simples e compat√≠vel
      const mimeType = MediaRecorder.isTypeSupported('audio/webm;codecs=opus')
        ? 'audio/webm;codecs=opus'
        : 'audio/webm';

      // MediaRecorder SIMPLES como Notion
      mediaRecorder = new MediaRecorder(stream, {
        mimeType,
        audioBitsPerSecond: 128000  // Qualidade padr√£o
      });

      // Reset estado
      audioChunks = [];
      transcript.value = '';
      audioBlob.value = null;
      hasAudio.value = false;
      error.value = null;
      startTime = new Date();

      // Event handlers SIMPLES
      mediaRecorder.ondataavailable = (e) => {
        if (e.data && e.data.size > 0) {
          audioChunks.push(e.data);
        }
      };

      mediaRecorder.onstop = () => {
        console.log('üõë Finalizando grava√ß√£o...');

        if (audioChunks.length > 0) {
          audioBlob.value = new Blob(audioChunks, { type: mimeType });
          hasAudio.value = true;
          console.log(`‚úÖ Grava√ß√£o finalizada: ${(audioBlob.value.size / 1024 / 1024).toFixed(2)}MB`);
        }

        isRecording.value = false;
        stopAudioAnalysis();

        // Parar stream
        stream.getTracks().forEach(track => track.stop());
      };

      mediaRecorder.onerror = (e) => {
        console.error('‚ùå Erro na grava√ß√£o:', e.error);
        error.value = `Erro na grava√ß√£o: ${e.error?.message || 'Desconhecido'}`;
        isRecording.value = false;
      };

      // Iniciar grava√ß√£o (m√©todo Notion)
      mediaRecorder.start(1000); // Chunks de 1 segundo
      isRecording.value = true;

      console.log('‚úÖ Grava√ß√£o iniciada com sucesso (m√©todo Notion)!');

    } catch (setupError) {
      console.error('‚ùå Erro ao configurar grava√ß√£o:', setupError.message);
      error.value = `Erro ao configurar grava√ß√£o: ${setupError.message}`;
      isRecording.value = false;
      throw setupError;
    }
  };

  const stopRecording = async () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop();
    }
    await cleanupStreams();
  };

  const clearTranscript = () => {
    transcript.value = '';
    error.value = null;
  };

  const getRecordingDuration = () => {
    if (!startTime) return 0;
    return Math.floor((new Date() - startTime) / 1000);
  };

  const transcribeAudio = async () => {
    if (!audioBlob.value) throw new Error('Nenhum √°udio dispon√≠vel.');
    isProcessing.value = true;
    error.value = null;

    try {
      const fileSizeMB = audioBlob.value.size / (1024 * 1024);
      const durationMinutes = getRecordingDuration() / 60;

      console.log(`üéµ Processando √°udio: ${fileSizeMB.toFixed(1)}MB, ${durationMinutes.toFixed(1)} minutos`);

      // Usa OpenAI Whisper como m√©todo principal
      if (OPENAI_API_KEY) {
        try {
          // Inicializa OpenAI se necess√°rio
          if (!openaiTranscription) {
            openaiTranscription = new OpenAITranscription();
            await openaiTranscription.initialize(OPENAI_API_KEY, OPENAI_ORG_ID);
          }

          console.log('üöÄ Usando OpenAI Whisper (oficial)');

          // Para arquivos grandes, usa chunking otimizado do Whisper
          if (fileSizeMB > 20 || durationMinutes > 25) {
            console.log('üì¶ Arquivo grande detectado, processando em chunks otimizados...');
            return await transcribeAudioInChunksWhisper();
          }

          // Para arquivos menores, usa m√©todo direto
          const transcriptText = await openaiTranscription.transcribe(audioBlob.value, {
            model: 'whisper-1',
            language: 'pt',
            temperature: 0.0,
            prompt: 'Esta √© uma transcri√ß√£o de uma reuni√£o corporativa em portugu√™s brasileiro. Incluir nomes pr√≥prios, termos t√©cnicos e evitar palavras de preenchimento.'
          });

          transcript.value = transcriptText;
          console.log('‚úÖ Transcri√ß√£o OpenAI conclu√≠da!');
          return transcriptText;

        } catch (openaiError) {
          console.warn('‚ö†Ô∏è Falha no OpenAI, tentando Gemini como fallback:', openaiError.message);

          // Tratamento espec√≠fico para diferentes erros de API
          if (openaiError.message.includes('quota') || openaiError.message.includes('insufficient_quota')) {
            error.value = 'Cota da API OpenAI excedida. Configure uma nova chave ou tente mais tarde.';
          } else if (openaiError.message.includes('API key')) {
            error.value = 'Chave da API OpenAI inv√°lida. Verifique sua configura√ß√£o.';
          } else if (openaiError.message.includes('rate')) {
            error.value = 'Limite de rate da OpenAI excedido. Aguarde alguns minutos.';
          }

          // Continua para Gemini como fallback
        }
      }

      // Fallback: Usa Gemini se Groq falhar ou n√£o estiver configurado
      console.log('üì± Usando Gemini como fallback...');

      // Para arquivos muito grandes, usa chunks
      if (fileSizeMB > 10 || durationMinutes > 15) {
        console.log('üì¶ Arquivo grande detectado, processando em chunks...');
        return await transcribeAudioInChunks();
      }

      // Para arquivos menores, usa m√©todo direto com Gemini
      const base64Audio = await blobToBase64(audioBlob.value);
      const prompt = `Voc√™ receber√° o √°udio de uma reuni√£o corporativa. Fa√ßa apenas a transcri√ß√£o completa e fiel em portugu√™s do Brasil, corrigindo erros de dic√ß√£o √≥bvios e removendo muletas ("√©√©", "ahn", "tipo"). Mantenha todos os nomes citados. Retorne apenas o texto transcrito, sem formata√ß√£o adicional ou coment√°rios.`;

      const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${GEMINI_API_KEY}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }, { inline_data: { mime_type: audioBlob.value.type || 'audio/webm', data: base64Audio } }] }],
          generationConfig: { temperature: 0.3, maxOutputTokens: 8000 }
        })
      });

      if (!response.ok) {
        const errorText = await response.text();
        console.error('Erro Gemini:', errorText);

        // Tratamento espec√≠fico para erros do Gemini
        if (response.status === 400 && errorText.includes('API key not valid')) {
          throw new Error('Chave da API Google (Gemini) inv√°lida. Verifique sua configura√ß√£o no .env');
        } else if (response.status === 429) {
          throw new Error('Limite de rate do Gemini excedido. Tente novamente em alguns minutos.');
        } else if (response.status === 403) {
          throw new Error('Acesso negado √† API Gemini. Verifique as permiss√µes da sua chave.');
        } else {
          throw new Error(`Falha na transcri√ß√£o Gemini: ${response.status} - ${errorText}`);
        }
      }

      const result = await response.json();
      const transcriptText = result.candidates?.[0]?.content?.parts?.[0]?.text;

      if (!transcriptText || !transcriptText.trim()) {
        throw new Error('Transcri√ß√£o vazia retornada.');
      }

      transcript.value = transcriptText;
      console.log('‚úÖ Transcri√ß√£o Gemini conclu√≠da');
      return transcriptText;

    } catch (e) {
      console.error('‚ùå Erro na transcri√ß√£o:', e);
      error.value = e.message;
      throw e;
    } finally {
      isProcessing.value = false;
    }
  };

  // Nova fun√ß√£o de chunking otimizada para Whisper
  const transcribeAudioInChunksWhisper = async () => {
    try {
      console.log('üîÑ Processando √°udio longo com chunks otimizados para Whisper...');

      // Chunks de 30 segundos conforme recomenda√ß√£o do Whisper
      const CHUNK_DURATION_SECONDS = 30;
      const OVERLAP_SECONDS = 2; // Overlap para evitar cortes no meio de palavras

      const chunks = await splitAudioIntoOptimizedChunks(audioBlob.value, CHUNK_DURATION_SECONDS, OVERLAP_SECONDS);
      console.log(`üì¶ Dividido em ${chunks.length} chunks de 30s com overlap`);

      let fullTranscript = '';
      let previousContext = ''; // Para manter contexto entre chunks

      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        const chunkSizeMB = chunk.size / (1024 * 1024);

        console.log(`üì§ Processando chunk ${i + 1}/${chunks.length} (${chunkSizeMB.toFixed(1)}MB)`);

        try {
          // Prompt contextual para manter continuidade
          const contextualPrompt = i === 0
            ? 'Esta √© uma transcri√ß√£o de uma reuni√£o corporativa em portugu√™s brasileiro. Incluir nomes pr√≥prios, termos t√©cnicos e evitar palavras de preenchimento.'
            : `Continua√ß√£o da reuni√£o. Contexto anterior: "${previousContext}". Manter consist√™ncia de nomes e termos.`;

          const chunkTranscript = await openaiTranscription.transcribe(chunk, {
            model: 'whisper-1',
            language: 'pt',
            temperature: 0.0,
            prompt: contextualPrompt
          });

          // Remove overlap duplicado (primeiras palavras se √© chunk > 0)
          let processedTranscript = chunkTranscript.trim();
          if (i > 0 && fullTranscript) {
            // Tenta remover sobreposi√ß√£o simples
            const lastWords = fullTranscript.split(' ').slice(-5).join(' ');
            const firstWords = processedTranscript.split(' ').slice(0, 5).join(' ');

            // Se h√° similaridade, remove as primeiras palavras do chunk atual
            if (lastWords.toLowerCase().includes(firstWords.toLowerCase().substring(0, 20))) {
              processedTranscript = processedTranscript.split(' ').slice(3).join(' ');
            }
          }

          if (fullTranscript && processedTranscript.trim()) {
            fullTranscript += ' ';
          }
          fullTranscript += processedTranscript.trim();

          // Atualiza contexto para pr√≥ximo chunk (√∫ltimas palavras)
          previousContext = fullTranscript.split(' ').slice(-10).join(' ');

          // Pequena pausa para evitar rate limiting
          if (i < chunks.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 500));
          }

        } catch (chunkError) {
          console.error(`‚ùå Erro no chunk ${i + 1}:`, chunkError);
          fullTranscript += ` [Erro na transcri√ß√£o do segmento ${i + 1}] `;
        }
      }

      if (!fullTranscript.trim()) {
        throw new Error('Nenhuma transcri√ß√£o foi obtida dos chunks.');
      }

      transcript.value = fullTranscript;
      console.log('‚úÖ Transcri√ß√£o em chunks finalizada');
      return fullTranscript;

    } catch (e) {
      console.error('‚ùå Erro ao processar √°udio em chunks:', e);
      throw new Error(`Erro ao processar √°udio longo: ${e.message}`);
    }
  };

  const transcribeAudioInChunks = async () => {
    const apiKey = GEMINI_API_KEY;
    const CHUNK_DURATION_MS = 10 * 60 * 1000; // 10 minutos por chunk

    try {
      console.log('üîÑ Processando √°udio longo em chunks...');

      const chunks = await splitAudioIntoChunks(audioBlob.value, CHUNK_DURATION_MS);
      console.log(`üì¶ Dividido em ${chunks.length} chunks`);

      let fullTranscript = '';
      const prompt = `Voc√™ receber√° parte do √°udio de uma reuni√£o corporativa. Fa√ßa apenas a transcri√ß√£o completa e fiel em portugu√™s do Brasil, corrigindo erros de dic√ß√£o √≥bvios e removendo muletas ("√©√©", "ahn", "tipo"). Mantenha todos os nomes citados. Retorne apenas o texto transcrito, sem formata√ß√£o adicional ou coment√°rios.`;

      for (let i = 0; i < chunks.length; i++) {
        const chunk = chunks[i];
        const chunkSizeMB = chunk.size / (1024 * 1024);

        console.log(`üì§ Processando chunk ${i + 1}/${chunks.length} (${chunkSizeMB.toFixed(1)}MB)`);

        try {
          let chunkTranscript = '';

          // Se chunk ainda √© muito grande, usar Files API
          if (chunkSizeMB > 10) {
            chunkTranscript = await transcribeChunkWithFilesAPI(chunk, prompt, apiKey, i + 1);
          } else {
            // Usar m√©todo inline para chunks menores
            const base64Audio = await blobToBase64(chunk);

            const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`, {
              method: 'POST',
              headers: { 'Content-Type': 'application/json' },
              body: JSON.stringify({
                contents: [{ parts: [{ text: prompt }, { inline_data: { mime_type: chunk.type || 'audio/webm', data: base64Audio } }] }],
                generationConfig: { temperature: 0.3, maxOutputTokens: 8000 }
              })
            });

            if (!resp.ok) {
              const errorText = await resp.text();
              console.error(`‚ùå Erro no chunk ${i + 1}:`, errorText);
              chunkTranscript = `[Erro na transcri√ß√£o do chunk ${i + 1}]`;
            } else {
              const data = await resp.json();
              chunkTranscript = data?.candidates?.[0]?.content?.parts?.map(p => p.text).join('\n') || `[Chunk ${i + 1} vazio]`;
            }
          }

          if (fullTranscript && chunkTranscript.trim()) {
            fullTranscript += '\n\n';
          }
          fullTranscript += chunkTranscript.trim();

          // Pequena pausa para evitar rate limiting
          if (i < chunks.length - 1) {
            await new Promise(resolve => setTimeout(resolve, 1000));
          }

        } catch (chunkError) {
          console.error(`‚ùå Erro no chunk ${i + 1}:`, chunkError);
          fullTranscript += `\n\n[Erro na transcri√ß√£o do segmento ${i + 1}: ${chunkError.message}]`;
        }
      }

      if (!fullTranscript.trim()) {
        throw new Error('Nenhuma transcri√ß√£o foi obtida dos chunks.');
      }

      transcript.value = fullTranscript;
      console.log('‚úÖ Transcri√ß√£o completa finalizada');
      return fullTranscript;

    } catch (e) {
      console.error('‚ùå Erro ao processar √°udio em chunks:', e);
      throw new Error(`Erro ao processar √°udio longo: ${e.message}`);
    }
  };

  const transcribeChunkWithFilesAPI = async (chunk, prompt, apiKey, chunkNumber) => {
    try {
      // 1. Upload do chunk
      const uploadData = new FormData();
      uploadData.append('file', chunk, `audio_chunk_${chunkNumber}.webm`);

      const uploadResp = await fetch(`https://generativelanguage.googleapis.com/upload/v1beta/files?key=${apiKey}`, {
        method: 'POST',
        body: uploadData
      });

      if (!uploadResp.ok) {
        const errorText = await uploadResp.text();
        throw new Error(`Erro no upload do chunk: ${errorText}`);
      }

      const uploadResult = await uploadResp.json();
      const fileUri = uploadResult.file.uri;

      // 2. Aguardar processamento
      await waitForFileProcessing(fileUri, apiKey);

      // 3. Transcrever
      const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-pro:generateContent?key=${apiKey}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{
            parts: [
              { text: prompt },
              { file_data: { mime_type: chunk.type || 'audio/webm', file_uri: fileUri } }
            ]
          }],
          generationConfig: { temperature: 0.3, maxOutputTokens: 8000 }
        })
      });

      if (!resp.ok) {
        const errorText = await resp.text();
        throw new Error(`Erro na transcri√ß√£o do chunk: ${errorText}`);
      }

      const data = await resp.json();
      const transcriptText = data?.candidates?.[0]?.content?.parts?.map(p => p.text).join('\n') || '';

      // 4. Limpar arquivo
      try {
        await fetch(`https://generativelanguage.googleapis.com/v1beta/files/${fileUri.split('/').pop()}?key=${apiKey}`, {
          method: 'DELETE'
        });
      } catch (e) {
        console.warn(`‚ö†Ô∏è N√£o foi poss√≠vel remover chunk tempor√°rio ${chunkNumber}:`, e);
      }

      return transcriptText;

    } catch (e) {
      throw new Error(`Erro ao processar chunk ${chunkNumber}: ${e.message}`);
    }
  };

  // Fun√ß√£o otimizada para dividir √°udio em chunks de 30s com overlap
  const splitAudioIntoOptimizedChunks = async (audioBlob, chunkDurationSeconds, overlapSeconds) => {
    const chunks = [];
    const AudioContextClass = window.AudioContext || window.webkitAudioContext;
    const audioContext = new AudioContextClass();

    try {
      // Decodifica o √°udio para obter informa√ß√µes precisas
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer.slice());

      const totalDurationSeconds = audioBuffer.duration;
      const chunkDurationMs = chunkDurationSeconds * 1000;
      const overlapMs = overlapSeconds * 1000;
      const effectiveChunkDuration = chunkDurationSeconds - overlapSeconds;

      const numChunks = Math.ceil(totalDurationSeconds / effectiveChunkDuration);

      console.log(`üéµ √Åudio total: ${(totalDurationSeconds / 60).toFixed(1)} min, dividindo em ${numChunks} chunks de ${chunkDurationSeconds}s`);

      // Calcula bytes por segundo para divis√£o aproximada
      const bytesPerSecond = audioBlob.size / totalDurationSeconds;

      for (let i = 0; i < numChunks; i++) {
        const startTimeSeconds = i * effectiveChunkDuration;
        const endTimeSeconds = Math.min(startTimeSeconds + chunkDurationSeconds, totalDurationSeconds);

        const startByte = Math.floor(startTimeSeconds * bytesPerSecond);
        const endByte = Math.floor(endTimeSeconds * bytesPerSecond);

        const chunkBlob = audioBlob.slice(startByte, endByte, audioBlob.type);
        chunks.push(chunkBlob);

        console.log(`üì¶ Chunk ${i + 1}: ${startTimeSeconds.toFixed(1)}-${endTimeSeconds.toFixed(1)}s (${(chunkBlob.size / 1024 / 1024).toFixed(1)}MB)`);
      }

      return chunks;

    } catch (e) {
      console.error('‚ùå Erro ao dividir √°udio em chunks otimizados:', e);
      // Fallback: usa divis√£o simples por tamanho
      const simpleChunkSize = Math.ceil(audioBlob.size / Math.ceil(audioBlob.size / (20 * 1024 * 1024))); // ~20MB por chunk
      const chunks = [];
      for (let i = 0; i < audioBlob.size; i += simpleChunkSize) {
        chunks.push(audioBlob.slice(i, i + simpleChunkSize, audioBlob.type));
      }
      return chunks;
    } finally {
      audioContext.close();
    }
  };

  const splitAudioIntoChunks = async (audioBlob, chunkDurationMs) => {
    const chunks = [];
    const AudioContextClass = window.AudioContext || window.webkitAudioContext;
    const audioContext = new AudioContextClass();

    try {
      // Decodifica o √°udio para obter informa√ß√µes
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioBuffer = await audioContext.decodeAudioData(arrayBuffer.slice());

      const totalDurationMs = audioBuffer.duration * 1000;
      const numChunks = Math.ceil(totalDurationMs / chunkDurationMs);

      console.log(`üéµ √Åudio total: ${(totalDurationMs / 1000 / 60).toFixed(1)} min, dividindo em ${numChunks} chunks`);

      // Para simplificar, vamos dividir o blob original por tempo estimado
      // (m√©todo aproximado mas funcional)
      const bytesPerMs = audioBlob.size / totalDurationMs;

      for (let i = 0; i < numChunks; i++) {
        const startTime = i * chunkDurationMs;
        const endTime = Math.min(startTime + chunkDurationMs, totalDurationMs);

        const startByte = Math.floor(startTime * bytesPerMs);
        const endByte = Math.floor(endTime * bytesPerMs);

        const chunkBlob = audioBlob.slice(startByte, endByte, audioBlob.type);
        chunks.push(chunkBlob);

        console.log(`üì¶ Chunk ${i + 1}: ${(startTime / 1000 / 60).toFixed(1)}-${(endTime / 1000 / 60).toFixed(1)} min (${(chunkBlob.size / 1024 / 1024).toFixed(1)}MB)`);
      }

      return chunks;

    } catch (e) {
      console.error('‚ùå Erro ao dividir √°udio:', e);
      // Fallback: retorna o √°udio original se falhar a divis√£o
      return [audioBlob];
    } finally {
      audioContext.close();
    }
  };

  const waitForFileProcessing = async (fileUri, apiKey, maxAttempts = 30) => {
    for (let i = 0; i < maxAttempts; i++) {
      const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/files/${fileUri.split('/').pop()}?key=${apiKey}`);
      const fileData = await resp.json();

      if (fileData.state === 'ACTIVE') {
        console.log('‚úÖ Arquivo processado e pronto');
        return;
      } else if (fileData.state === 'FAILED') {
        throw new Error('Falha no processamento do arquivo');
      }

      console.log(`‚è≥ Aguardando processamento... (${i + 1}/${maxAttempts})`);
      await new Promise(resolve => setTimeout(resolve, 2000)); // Aguarda 2s
    }

    throw new Error('Timeout no processamento do arquivo');
  };

  const generateSummaryFromTranscript = async () => {
    if (!transcript.value) throw new Error('Nenhuma transcri√ß√£o dispon√≠vel.');
    isProcessing.value = true;
    error.value = null;
    try {
      const apiKey = import.meta.env.VITE_GOOGLE_API_KEY;
      if (!apiKey) throw new Error('Chave da API ausente (.env).');

      const prompt = `Voc√™ receber√° a transcri√ß√£o de uma reuni√£o corporativa. Produza APENAS o JSON final (sem markdown, sem coment√°rios) seguindo as regras abaixo rigorosamente.\n\nOBJETIVO: Gerar um RESUMO ESTRUTURADO com t√≠tulo inteligente e pontos principais organizados de forma direta e pr√°tica.\n\nREGRAS:\n1. titulo_reuniao: Crie um t√≠tulo descritivo e inteligente baseado no contexto da reuni√£o (ex: "Mentoria de Desenvolvimento de Carreira", "Planejamento Sprint Q4", etc.)\n2. contexto_e_objetivo: Breve contexto da reuni√£o e objetivo principal (1-2 frases).\n3. participantes: Array com TODOS os nomes mencionados na reuni√£o. Se n√£o houver nomes espec√≠ficos, usar ["Participantes n√£o identificados"].\n4. pontos_principais: Array de objetos com os t√≥picos espec√≠ficos discutidos. QUEBRE EM SUBT√ìPICOS MENORES E ESPEC√çFICOS. Cada objeto deve ter:\n   - "subtitulo": nome/t√≠tulo espec√≠fico do subt√≥pico (ex: "Melhorar tela de login", "C√≥digo √∫nico de oferta", "Problema com autentica√ß√£o", "Configurar servidor de desenvolvimento")\n   - "pontos_abordados": array com 2-4 pontos espec√≠ficos do que foi falado sobre esse subt√≥pico (ex: ["jesiel falou para mudar as cores do fundo", "ajustar as bordas para ficar mais arredondadas"])\n5. action_items: TODAS as a√ß√µes mencionadas. Formato: { "descricao": "a√ß√£o espec√≠fica", "responsavel": "nome ou 'A definir'", "prazo": "prazo mencionado ou 'N√£o definido'", "concluida": false }\n6. decisoes_tomadas: Array com decis√µes concretas tomadas durante a reuni√£o.\n7. proximos_passos: Pr√≥ximas etapas estrat√©gicas mencionadas.\n\nIMPORTANTE: Para os pontos_principais, QUEBRE EM MUITOS SUBT√ìPICOS ESPEC√çFICOS ao inv√©s de poucos t√≥picos grandes. Cada subt√≥pico deve ter entre 2-4 pontos abordados. Seja espec√≠fico e direto. Capture exatamente o que foi dito, incluindo quem falou o qu√™. Use linguagem natural e direta.\n\nEXEMPLO CORRETO de pontos_principais (muitos subt√≥picos espec√≠ficos):\n[\n  {\n    "subtitulo": "Cores da tela de login",\n    "pontos_abordados": [\n      "jesiel falou para mudar as cores do fundo",\n      "usar tons mais escuros"\n    ]\n  },\n  {\n    "subtitulo": "Bordas dos elementos",\n    "pontos_abordados": [\n      "ajustar as bordas para ficar mais arredondadas",\n      "aplicar border-radius de 8px"\n    ]\n  },\n  {\n    "subtitulo": "Tipografia dos bot√µes",\n    "pontos_abordados": [\n      "revisar tipografia dos bot√µes",\n      "usar fonte maior para melhor legibilidade"\n    ]\n  },\n  {\n    "subtitulo": "Localiza√ß√£o do c√≥digo √∫nico",\n    "pontos_abordados": [\n      "est√° presente no book",\n      "seguindo a documenta√ß√£o o c√≥digo √∫nico j√° vem do book por√©m precisa achar ele"\n    ]\n  },\n  {\n    "subtitulo": "Verifica√ß√£o com backend",\n    "pontos_abordados": [\n      "verificar com o time de backend onde est√° localizado",\n      "agendar reuni√£o para entender a estrutura"\n    ]\n  }\n]\n\nN√ÉO FA√áA subt√≥picos muito grandes com muitos pontos. PREFIRA v√°rios subt√≥picos espec√≠ficos e menores.\n\nFORMATO EXATO DO RETORNO (JSON √öNICO):\n{\n  "titulo_reuniao": "...",\n  "contexto_e_objetivo": "...",\n  "participantes": ["..."],\n  "pontos_principais": [\n    {\n      "subtitulo": "...",\n      "pontos_abordados": ["...", "...", "..."]\n    }\n  ],\n  "action_items": [{"descricao": "...", "responsavel": "...", "prazo": "...", "concluida": false}],\n  "decisoes_tomadas": ["..."],\n  "proximos_passos": ["..."],\n  "formato_estruturado": true\n}\n\nTranscri√ß√£o da reuni√£o:\n${transcript.value}\n\nRetorne somente o JSON completo e detalhado no formato estruturado.`;

      const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }],
          generationConfig: { temperature: 0.6, maxOutputTokens: 8000 }
        })
      });
      if (!resp.ok) throw new Error('Erro da API: ' + await resp.text());
      const data = await resp.json();
      const raw = data?.candidates?.[0]?.content?.parts?.map(p => p.text).join('\n') || '';
      console.log('üîç Resposta bruta da IA (nova grava√ß√£o):', raw);

      const match = raw.match(/\{[\s\S]*\}/);
      if (!match) throw new Error('JSON n√£o encontrado na resposta da IA.');
      const parsed = JSON.parse(match[0]);
      console.log('üìã JSON parsed (nova grava√ß√£o):', parsed);

      // Suporte para formato estruturado (novo) e formato antigo (compatibilidade)
      if (parsed.formato_estruturado) {
        return {
          titulo_reuniao: parsed.titulo_reuniao || 'Reuni√£o',
          contexto_e_objetivo: parsed.contexto_e_objetivo || '',
          participantes: parsed.participantes || ['Participantes n√£o identificados'],
          pontos_principais: parsed.pontos_principais || [],
          action_items: parsed.action_items || [],
          decisoes_tomadas: parsed.decisoes_tomadas || [],
          proximos_passos: parsed.proximos_passos || [],
          data_reuniao: new Date().toISOString(),
          duracao_minutos: Math.round(getRecordingDuration() / 60 * 10) / 10,
          fonte: 'Transcri√ß√£o via Whisper + An√°lise via Gemini',
          formato_estruturado: true
        };
      } else {
        // Formato antigo (compatibilidade)
        return {
          contexto: parsed.contexto || '',
          participantes: parsed.participantes || ['Participantes n√£o identificados'],
          pontos_discutidos: parsed.pontos_discutidos || [],
          decisoes_tomadas: parsed.decisoes_tomadas || [],
          tarefas_e_acoes: parsed.tarefas_e_acoes || [],
          proximos_passos: parsed.proximos_passos || [],
          observacoes: parsed.observacoes || [],
          data_reuniao: new Date().toISOString(),
          duracao_minutos: Math.round(getRecordingDuration() / 60 * 10) / 10,
          fonte: 'Transcri√ß√£o via Whisper + An√°lise via Gemini'
        };
      }
    } catch (e) {
      error.value = e.message;
      throw e;
    } finally {
      isProcessing.value = false;
    }
  };

  // Nova fun√ß√£o para regenerar resumo de reuni√µes existentes no formato estruturado
  const regenerateSummaryWithNewFormat = async (transcriptText, originalDuration = 0) => {
    if (!transcriptText) throw new Error('Nenhuma transcri√ß√£o fornecida.');
    isProcessing.value = true;
    error.value = null;

    try {
      const apiKey = import.meta.env.VITE_GOOGLE_API_KEY;
      if (!apiKey) throw new Error('Chave da API ausente (.env).');

      const prompt = `Voc√™ receber√° a transcri√ß√£o de uma reuni√£o corporativa. Produza APENAS o JSON final (sem markdown, sem coment√°rios) seguindo as regras abaixo rigorosamente.\n\nOBJETIVO: Gerar um RESUMO ESTRUTURADO com t√≠tulo inteligente e pontos principais organizados de forma direta e pr√°tica.\n\nREGRAS:\n1. titulo_reuniao: Crie um t√≠tulo descritivo e inteligente baseado no contexto da reuni√£o (ex: "Mentoria de Desenvolvimento de Carreira", "Planejamento Sprint Q4", etc.)\n2. contexto_e_objetivo: Breve contexto da reuni√£o e objetivo principal (1-2 frases).\n3. participantes: Array com TODOS os nomes mencionados na reuni√£o. Se n√£o houver nomes espec√≠ficos, usar ["Participantes n√£o identificados"].\n4. pontos_principais: Array de objetos com os t√≥picos espec√≠ficos discutidos. QUEBRE EM SUBT√ìPICOS MENORES E ESPEC√çFICOS. Cada objeto deve ter:\n   - "subtitulo": nome/t√≠tulo espec√≠fico do subt√≥pico (ex: "Melhorar tela de login", "C√≥digo √∫nico de oferta", "Problema com autentica√ß√£o", "Configurar servidor de desenvolvimento")\n   - "pontos_abordados": array com 2-4 pontos espec√≠ficos do que foi falado sobre esse subt√≥pico (ex: ["jesiel falou para mudar as cores do fundo", "ajustar as bordas para ficar mais arredondadas"])\n5. action_items: TODAS as a√ß√µes mencionadas. Formato: { "descricao": "a√ß√£o espec√≠fica", "responsavel": "nome ou 'A definir'", "prazo": "prazo mencionado ou 'N√£o definido'", "concluida": false }\n6. decisoes_tomadas: Array com decis√µes concretas tomadas durante a reuni√£o.\n7. proximos_passos: Pr√≥ximas etapas estrat√©gicas mencionadas.\n\nIMPORTANTE: Para os pontos_principais, QUEBRE EM MUITOS SUBT√ìPICOS ESPEC√çFICOS ao inv√©s de poucos t√≥picos grandes. Cada subt√≥pico deve ter entre 2-4 pontos abordados. Seja espec√≠fico e direto. Capture exatamente o que foi dito, incluindo quem falou o qu√™. Use linguagem natural e direta.\n\nEXEMPLO CORRETO de pontos_principais (muitos subt√≥picos espec√≠ficos):\n[\n  {\n    "subtitulo": "Cores da tela de login",\n    "pontos_abordados": [\n      "jesiel falou para mudar as cores do fundo",\n      "usar tons mais escuros"\n    ]\n  },\n  {\n    "subtitulo": "Bordas dos elementos",\n    "pontos_abordados": [\n      "ajustar as bordas para ficar mais arredondadas",\n      "aplicar border-radius de 8px"\n    ]\n  },\n  {\n    "subtitulo": "Tipografia dos bot√µes",\n    "pontos_abordados": [\n      "revisar tipografia dos bot√µes",\n      "usar fonte maior para melhor legibilidade"\n    ]\n  },\n  {\n    "subtitulo": "Localiza√ß√£o do c√≥digo √∫nico",\n    "pontos_abordados": [\n      "est√° presente no book",\n      "seguindo a documenta√ß√£o o c√≥digo √∫nico j√° vem do book por√©m precisa achar ele"\n    ]\n  },\n  {\n    "subtitulo": "Verifica√ß√£o com backend",\n    "pontos_abordados": [\n      "verificar com o time de backend onde est√° localizado",\n      "agendar reuni√£o para entender a estrutura"\n    ]\n  }\n]\n\nN√ÉO FA√áA subt√≥picos muito grandes com muitos pontos. PREFIRA v√°rios subt√≥picos espec√≠ficos e menores.\n\nFORMATO EXATO DO RETORNO (JSON √öNICO):\n{\n  "titulo_reuniao": "...",\n  "contexto_e_objetivo": "...",\n  "participantes": ["..."],\n  "pontos_principais": [\n    {\n      "subtitulo": "...",\n      "pontos_abordados": ["...", "...", "..."]\n    }\n  ],\n  "action_items": [{"descricao": "...", "responsavel": "...", "prazo": "...", "concluida": false}],\n  "decisoes_tomadas": ["..."],\n  "proximos_passos": ["..."],\n  "formato_estruturado": true\n}\n\nTranscri√ß√£o da reuni√£o:\n${transcriptText}\n\nRetorne somente o JSON completo e detalhado no formato estruturado.`;

      const resp = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?key=${apiKey}`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          contents: [{ parts: [{ text: prompt }] }],
          generationConfig: { temperature: 0.6, maxOutputTokens: 8000 }
        })
      });

      if (!resp.ok) throw new Error('Erro da API: ' + await resp.text());

      const data = await resp.json();
      const raw = data?.candidates?.[0]?.content?.parts?.map(p => p.text).join('\n') || '';
      console.log('üîç Resposta bruta da IA (regenera√ß√£o):', raw);

      const match = raw.match(/\{[\s\S]*\}/);
      if (!match) throw new Error('JSON n√£o encontrado na resposta da IA.');
      const parsed = JSON.parse(match[0]);
      console.log('üìã JSON parsed (regenera√ß√£o):', parsed);

      const result = {
        titulo_reuniao: parsed.titulo_reuniao || 'Reuni√£o',
        contexto_e_objetivo: parsed.contexto_e_objetivo || '',
        participantes: parsed.participantes || ['Participantes n√£o identificados'],
        pontos_principais: parsed.pontos_principais || [],
        action_items: parsed.action_items || [],
        decisoes_tomadas: parsed.decisoes_tomadas || [],
        proximos_passos: parsed.proximos_passos || [],
        data_reuniao: new Date().toISOString(),
        duracao_minutos: originalDuration,
        fonte: 'Regenera√ß√£o: Transcri√ß√£o via Whisper + An√°lise via Gemini',
        formato_estruturado: true,
        // Limpar campos do formato antigo para evitar conflitos
        pontos_discutidos: undefined,
        tarefas: undefined,
        tarefas_e_acoes: undefined,
        geral: undefined,
        observacoes: undefined
      };

      console.log('‚úÖ Resultado final da regenera√ß√£o:', result);
      return result;

    } catch (e) {
      error.value = e.message;
      throw e;
    } finally {
      isProcessing.value = false;
    }
  };


  const setupElectronListener = () => {
    if (window.electronAPI && window.electronAPI.onStartRecording) {
      removeElectronListener = window.electronAPI.onStartRecording(() => {
        if (!isRecording.value) startRecording();
      });
    }
  };


  // Fun√ß√£o para configurar a API key do OpenAI
  const setOpenAIApiKey = (apiKey, organizationId = null) => {
    if (apiKey && apiKey.trim()) {
      OPENAI_API_KEY = apiKey.trim();
      OPENAI_ORG_ID = organizationId;

      // Reset da inst√¢ncia para for√ßar reinicializa√ß√£o
      openaiTranscription = null;

      console.log('üîë OpenAI API Key configurada');
      return true;
    }
    OPENAI_API_KEY = '';
    OPENAI_ORG_ID = null;
    openaiTranscription = null;
    return false;
  };

  // Fun√ß√£o para testar conex√£o OpenAI
  const testOpenAIConnection = async (apiKey, organizationId = null) => {
    try {
      const testOpenAI = new OpenAITranscription();
      await testOpenAI.initialize(apiKey, organizationId);
      return await testOpenAI.testConnection();
    } catch (error) {
      console.error('‚ùå Teste OpenAI falhou:', error);
      return false;
    }
  };

  // Fun√ß√£o para estimar custo (apenas OpenAI Whisper)
  const estimateTranscriptionCost = (durationMinutes) => {
    if (openaiTranscription) {
      return openaiTranscription.estimateCost(durationMinutes);
    }
    // Estimativa manual se a inst√¢ncia n√£o estiver dispon√≠vel
    const costUSD = durationMinutes * 0.006;
    return {
      model: 'whisper-1',
      durationMinutes,
      durationHours: (durationMinutes / 60).toFixed(2),
      costUSD: costUSD.toFixed(4),
      costBRL: (costUSD * 5.5).toFixed(2)
    };
  };

  // Fun√ß√£o para resetar estado de captura
  const resetCaptureState = () => {
    audioCaptureType.value = 'unknown';
    isCapturingFullMeeting.value = false;
    audioSources.value = [];
    isCapturingInput.value = false;
    isCapturingOutput.value = false;
    audioQuality.value = { input: 0, output: 0 };
    stopAudioAnalysis();
  };

  onMounted(() => {
    checkSupport();
    setupElectronListener();
  });

  onUnmounted(async () => {
    if (mediaRecorder && mediaRecorder.state === 'recording') {
      mediaRecorder.stop();
    }
    if (removeElectronListener) {
      removeElectronListener();
    }
    await cleanupStreams();
  });

  return {
    isRecording,
    isProcessing,
    isSupported,
    transcript,
    error,
    hasAudio,
    audioBlob,
    // Estado da captura de √°udio
    audioCaptureType,
    isCapturingFullMeeting,
    audioSources,
    isCapturingInput,
    isCapturingOutput,
    audioQuality,
    detectedMeetingApp,
    // Fun√ß√µes
    startRecording,
    stopRecording,
    clearTranscript,
    transcribeAudio,
    generateSummaryFromTranscript,
    regenerateSummaryWithNewFormat,
    getRecordingDuration,
    transcribeAudioInChunks,
    setOpenAIApiKey,
    testOpenAIConnection,
    estimateTranscriptionCost,
    hasOpenAIConfigured: () => !!OPENAI_API_KEY
  };
}

async function blobToBase64(blob) {
  const buffer = await blob.arrayBuffer();
  let binary = '';
  const bytes = new Uint8Array(buffer);
  for (let i = 0; i < bytes.length; i++) binary += String.fromCharCode(bytes[i]);
  return btoa(binary);
}
